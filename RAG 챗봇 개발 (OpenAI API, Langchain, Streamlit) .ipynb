{"cells":[{"cell_type":"markdown","metadata":{"id":"-QPBTYH8lsFm"},"source":["# RAG 챗봇 개발 (OpenAI API, Langchain, Streamlit)\n","\n","## 폴더 구조\n","\n","```\n","ROOT/\n","- data/ --> 문서가 들어있는 폴더\n","- .env --> OpenAI API 키가 저장되어 있는 파일.\n","- db/ --> 벡터DB (처음에는 샘플 문서가 저장)\n","- utils.py --> RAG 챗봇 로직 코드\n","- rag.py --> 챗봇 (과거기억 못합) UI 코드\n","- history_rag.py --> 챗봇 (과거기억함) UI 코드\n","- RAG 챗봇 개발 (OpenAI API, Langchain, Streamlit).ipynb --> 현재 노트북 파일\n","```\n","\n","## 사용 툴\n","\n","- OpenAI API\n","    - 임베딩 생성 모델과 LLM을 제공합니다.\n","    - 모델은 OpenAI의 서버에 올려져 있으므로 민감한 문서나 내용을 넘기지 않도록 주의가 필요합니다.\n","    - 유료라서 OpenAI API 계정에 돈을 충전해야 합니다. $1면 충분합니다. ⚠ ChatGPT Plus와는 다릅니다. ⚠\n","- Langchain\n","    - LLM 앱 개발을 위한 프레임워크입니다.\n","- ChromaDB\n","    - 문서를 저장할 오픈소스 벡터DB입니다.\n","- Streamlit\n","    - 챗봇 UI를 생성하기 위해 사용할 프레임워크입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fXsNOuAZnJky"},"source":["## 환경설정\n","\n","구글 colab CPU 환경에서 실행하는 것을 가정합니다.\n","\n","1. .env 파일에 `OPENAI_API_KEY`가 있어야 합니다.\n","2. 구글 드라이브를 마운트합니다. (밑 코드 실행)\n","3. 필요한 패키지를 다운받습니다. (밑 코드 실행)"]},{"cell_type":"markdown","metadata":{"id":"htZC936hMVIu"},"source":["구글 드라이브를 연결합니다."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19165,"status":"ok","timestamp":1716899563344,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"22XqhlOhL1Iv","outputId":"bad9f63f-f4d4-4d65-98fc-5db3f520d492"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"3m3TDA99MFsp"},"source":["`ROOT`변수에 `.env` 파일이 들어있는 폴더 경로를 입력합니다."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1716899743212,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"wUlxAHsTst-E"},"outputs":[],"source":["ROOT = \"/content/drive/MyDrive/AI대회_참고자료\" # 수정필요"]},{"cell_type":"markdown","metadata":{"id":"hGGbYz8SNM1e"},"source":["`ROOT`에 입력한 경로로 이동합니다."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716899814688,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"cn3hSxNPM0ro","outputId":"20c837ba-5e9e-441f-8e87-a382ff166a77"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/AI대회_참고자료\n"]}],"source":["%cd $ROOT"]},{"cell_type":"markdown","metadata":{"id":"KTEu1F4UNVd0"},"source":["필요한 패키지를 다운받습니다."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46429,"status":"ok","timestamp":1716899791048,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"KFwGRmi8q1fN","outputId":"9bfb6229-461c-4505-81d1-b5158f0572a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/973.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/973.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma pypdf python-dotenv langchain-openai streamlit"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4414,"status":"ok","timestamp":1716899812217,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"N7RO0W18xpku","outputId":"678135e7-e7ba-4c1a-81ec-b372bb9c1f38"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","added 22 packages from 22 contributors and audited 22 packages in 2.185s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n","  run `npm audit fix` to fix them, or `npm audit` for details\n","\u001b[K\u001b[?25h"]}],"source":["!npm install localtunnel"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1716899936331,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"R6VrOv-yq-N5","outputId":"2ad1424b-4ae4-4ff9-bff0-5e7efea45674"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","from dotenv import load_dotenv\n","\n","# .env 파일에 저장된 환경변수를 불러옵니다.\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{"id":"fvArA65nrFIn"},"source":["## 문서를 벡터 DB에 저장하기\n","\n","`ROOT/data` 폴더에 PDF 문서들이 저장되어 있다고 가정합니다.\n","\n","다른 경우는 [공식 문서](https://python.langchain.com/v0.1/docs/integrations/document_loaders/)를 참고하면 됩니다.\n","- PDF가 아닌 문서를 다루는 경우 eg. [MS word](https://python.langchain.com/v0.1/docs/integrations/document_loaders/microsoft_word/)\n","- 웹에서 문서를 다운받는 경우"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":505,"status":"ok","timestamp":1716899950593,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"qmoQHgynraUl"},"outputs":[],"source":["pdf_folder_path = \"./data\""]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":50625,"status":"ok","timestamp":1716900464109,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"H8RVcGNRrXte"},"outputs":[],"source":["# 폴더 속 모든 PDF 파일 로드\n","from langchain.document_loaders import PyPDFDirectoryLoader\n","\n","loader = PyPDFDirectoryLoader(pdf_folder_path)\n","docs = loader.load()\n","print(f\"{len(docs)} 페이지 로드\")"]},{"cell_type":"markdown","metadata":{"id":"0EC4lePDzzCb"},"source":["문서를 1000자(200자는 중복)로 자르고 임베딩해서 벡터DB에 저장합니다.\n","\n","`persist_directory`에 벡터DB를 저장할 위치를 입력합니다. 예시에서는 `./db` 폴더로 설정되어 있습니다."]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":9596,"status":"ok","timestamp":1716900649020,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"pkGzWqcPJukr"},"outputs":[],"source":["### OpenAI Embedding 사용해서 문서를 임베딩 후 Chroma 벡터DB에 저장.\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","\n","embedder = OpenAIEmbeddings()\n","\n","vectorstore = Chroma.from_documents(\n","    documents=splits,\n","    embedding=embedder,\n","    persist_directory=\"./db\"\n",")\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"markdown","metadata":{"id":"16-NpPfcvyPu"},"source":["## Streamlit으로 챗봇 UI 구현하기\n","\n","### 방법\n","\n","1. 미리 모든 문서를 청크로 분리하고 임베딩해서 벡터DB에 저장합니다.\n","2. 사용자한테 질문을 입력받습니다.\n","3. 질문과 비슷한 문서 청크를 벡터DB에서 찾아 가져옵니다.\n","4. 가져온 문서 청크를 기반으로 사용자 질문에 대한 답변을 LLM이 생성합니다.\n","\n","streamlit으로 `rag.py` 코드를 실행하면 챗봇 앱을 시작합니다.\n","\n","마지막 줄에 있는 링크로 들어갑니다. `Tunnel Password`는 첫 번째 줄에 출력된 ip 주소 입니다."]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34608,"status":"ok","timestamp":1716902150809,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"0CFEzWduxtnW","outputId":"0f9e9ff9-13f5-49da-e4a9-e0f79357273d"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.74.109.162\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.322s\n","your url is: https://free-tips-lie.loca.lt\n"]}],"source":["!streamlit run rag.py \\\n","&>/content/logs.txt \\\n","& npx localtunnel --port 8501 \\\n","& curl ipv4.icanhazip.com"]},{"cell_type":"markdown","metadata":{"id":"jYDGLXDzxu0Q"},"source":["## Streamlit으로 챗봇 UI 구현하기 (과거 질문답변 기억)\n","\n","### 방법\n","\n","1. 미리 모든 문서를 청크로 분리하고 임베딩해서 벡터DB에 저장합니다.\n","2. 사용자한테 질문을 입력받습니다.\n","3. 과거 질문답변 내역 없이도 질문에 답변할 수 있도록 새로운 질문으로 만들어달라고 LLM에 요청합니다.\n","3. 새로운 질문과 비슷한 문서 청크를 벡터DB에서 찾아 가져옵니다.\n","4. 가져온 문서 청크와 과거 질문답변 내역을 기반으로 사용자 질문에 대한 답변을 LLM이 생성합니다."]},{"cell_type":"markdown","metadata":{"id":"Jm-3VUjMTMfe"},"source":["streamlit으로 `history_rag.py` 코드를 실행하면 챗봇 앱을 시작합니다.\n","\n","마지막 줄에 있는 링크로 들어갑니다. `Tunnel Password`는 첫 번째 줄에 출력된 ip 주소 입니다."]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81031,"status":"ok","timestamp":1716902270852,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"0jyycYalx0tv","outputId":"7dd113b6-e01b-4350-b15a-2704f01c9241"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.74.109.162\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.555s\n","your url is: https://tall-beers-laugh.loca.lt\n"]}],"source":["!streamlit run history_rag.py \\\n","&>/content/logs.txt \\\n","& npx localtunnel --port 8501 \\\n","& curl ipv4.icanhazip.com"]},{"cell_type":"markdown","metadata":{"id":"PoKTGlPpWxXa"},"source":["## (부록) 코드 파일 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1716902178352,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"nmMl2Vx7v4du","outputId":"24c2bbce-559e-41b1-c39d-389ab3a8bf2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting utils.py\n"]}],"source":["%%writefile utils.py\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","from langchain_openai import ChatOpenAI\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_core.prompts import PromptTemplate\n","\n","from langchain.chains import create_history_aware_retriever\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.chains import create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","\n","# LLM 로드\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")\n","\n","vectorstore = Chroma(\n","    persist_directory=\"./db\",\n","    embedding_function=OpenAIEmbeddings())\n","# Retrieve and generate using the relevant snippets of the blog.\n","retriever = vectorstore.as_retriever()\n","\n","def get_rag_chain():\n","\n","    # template = \"\"\"제공된 맥락을 사용하여 질문에 답변하십시오.\n","    # 답을 모르겠다면 모른다고 말하고, 답을 지어내지 마십시오.\n","    # 답변을 간결하게 작성하십시오.\n","\n","    # {context}\n","\n","    # 질문: {question}\n","\n","    # 답변:\"\"\"\n","\n","    template = \"\"\"You are an assistant for question-answering tasks. \\\n","    Use the following pieces of retrieved context to answer the question. \\\n","    If you don't know the answer, just say that you don't know. \\\n","\n","    {context}\n","\n","    질문: {question}\n","\n","    답변:\"\"\"\n","\n","    custom_rag_prompt = PromptTemplate.from_template(template)\n","\n","    def format_docs(docs):\n","        return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","    rag_chain = (\n","        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","        | custom_rag_prompt\n","        | llm\n","        | StrOutputParser()\n","    )\n","    return rag_chain\n","\n","def get_history_rag_chain():\n","\n","    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n","    which might reference context in the chat history, formulate a standalone question \\\n","    which can be understood without the chat history. Do NOT answer the question, \\\n","    just reformulate it if needed and otherwise return it as is.\"\"\"\n","    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", contextualize_q_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", \"{input}\"),\n","        ]\n","    )\n","    history_aware_retriever = create_history_aware_retriever(\n","        llm, retriever, contextualize_q_prompt\n","    )\n","\n","    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n","    Use the following pieces of retrieved context to answer the question. \\\n","    If you don't know the answer, just say that you don't know. \\\n","\n","    {context}\"\"\"\n","\n","    qa_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", qa_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", \"{input}\"),\n","        ]\n","    )\n","\n","    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n","\n","    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n","    return rag_chain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1716902039956,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"1pbcCqQSwwAZ","outputId":"0fb4573f-c6c2-46f8-a0f1-d324bffd7b10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting rag.py\n"]}],"source":["%%writefile rag.py\n","import streamlit as st\n","from utils import get_rag_chain\n","\n","rag_chain = get_rag_chain()\n","\n","st.title(\"📝 PDF 기반 QA 챗봇\")\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"질문에 답변해드립니다.\"}]\n","\n","for msg in st.session_state.messages:\n","    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","if prompt := st.chat_input():\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    st.chat_message(\"user\").write(prompt)\n","\n","    response = st.chat_message(\"assistant\").write_stream(rag_chain.stream(prompt))\n","    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1716902183773,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"OWdV15QTUV-k","outputId":"1a91f9a5-9733-4907-e881-ef9625f22312"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting history_rag.py\n"]}],"source":["%%writefile history_rag.py\n","import streamlit as st\n","from langchain_community.chat_message_histories import (\n","    StreamlitChatMessageHistory,\n",")\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","from utils import get_history_rag_chain\n","\n","rag_chain = get_history_rag_chain()\n","\n","history = StreamlitChatMessageHistory(key=\"test\")\n","\n","if len(history.messages) == 0 or st.sidebar.button(\"초기화\"):\n","    history.clear()\n","    history.add_ai_message(\"문서 내용에 대해 질문하면 답변해드립니다.\")\n","\n","\n","conversational_rag_chain = RunnableWithMessageHistory(\n","    rag_chain,\n","    # get_session_history,\n","    lambda session_id: history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"chat_history\",\n","    output_messages_key=\"answer\",\n",")\n","\n","for msg in history.messages:\n","    st.chat_message(msg.type).write(msg.content)\n","\n","if prompt := st.chat_input():\n","    st.chat_message(\"human\").write(prompt)\n","\n","    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n","    config = {\"configurable\": {\"session_id\": \"test\"}}\n","    with st.spinner(\"답변 확인 중...\"):\n","        response = conversational_rag_chain.invoke({\"input\": prompt}, config)\n","    st.chat_message(\"ai\").write(response[\"answer\"])"]},{"cell_type":"markdown","metadata":{"id":"itTK4wGEW_6R"},"source":["# 코드 설명"]},{"cell_type":"markdown","metadata":{"id":"Pg430LyJXLzH"},"source":["## RAG 챗봇 (과거 질문답변 기억 못 함)"]},{"cell_type":"markdown","metadata":{"id":"ZRYJxSiPXpKF"},"source":["미리 만들어 놓은 벡터DB를 불러옵니다. 사용자 질문과 유사한 문서를 찾아올 `retriever`도 설정합니다.\n","\n","벡터DB는 `db/` 폴더에 저장되어 있다고 가정합니다."]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1716902646330,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"3tkEdFiAXTJQ"},"outputs":[],"source":["from langchain_chroma import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","\n","vectorstore = Chroma(\n","    persist_directory=\"./db\",\n","    embedding_function=OpenAIEmbeddings())\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"markdown","metadata":{"id":"JYocN6PPYPK7"},"source":["답변을 생성하는데 사용할 LLM을 불러옵니다.\n","\n","OpenAI API를 사용해서 gpt-3.5 모델을 불러옵니다."]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":414,"status":"ok","timestamp":1716902787146,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"2vBfCDJBYMNy"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")"]},{"cell_type":"markdown","metadata":{"id":"1Wb6MVL5YgjL"},"source":["LLM에게 넘겨줄 프롬프트 형식입니다.\n","\n","- `context`: 문맥 정보를 줄 문서가 들어갑니다.\n","- `question`: 사용자가 입력한 질문이 들어갑니다.\n"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":527,"status":"ok","timestamp":1716902950184,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"6aymWdARX028"},"outputs":[],"source":["from langchain_core.prompts import PromptTemplate\n","\n","template = \"\"\"You are an assistant for question-answering tasks. \\\n","Use the following pieces of retrieved context to answer the question. \\\n","If you don't know the answer, just say that you don't know. \\\n","\n","{context}\n","\n","질문: {question}\n","\n","답변:\"\"\"\n","\n","custom_rag_prompt = PromptTemplate.from_template(template)"]},{"cell_type":"markdown","metadata":{"id":"1TOXteweZHZ2"},"source":["- 사용자의 입력을 받으면\n","- `retriever | format_docs`: 벡터DB에서 문서 가져오기 | 가져온 문서 합치기\n","- `custom_rag_prompt`: 프롬프트 만들기\n","- `llm`: LLM으로 생성하기\n","- `StrOutputParser()`: 출력하기\n","\n","의 단계를 연결해서 `rag_chain`을 만듭니다."]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716902968402,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"QzIMja0bYwT0"},"outputs":[],"source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | custom_rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{"id":"fIqW0aySaDYX"},"source":["답변을 잘 하는지 확인해 봅시다."]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13166,"status":"ok","timestamp":1716903011931,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"Df6e00OQZDm6","outputId":"8fef38e1-a477-4978-983f-eb904ac9f198"},"outputs":[{"name":"stdout","output_type":"stream","text":["- 목차\n","- 서론\n","  - 본 백서 작성의 배경\n","    - LLM을 왜 평가하는가?\n","  - 백서의 대상 독자\n","  - LLM 평가의 프레임워크와 용어 정리\n","- 언어 모델 평가의 전체 모습\n","  - 본 백서에서 제안하는 체계\n","- LLM 모델의 평가 방법\n","  - What to evaluate: 평가할 측면\n","    - 기초 언어 능력\n","      - 언어 이해\n","      - 자연어 추론\n","    - 응용 능력\n","      - 지식·질의응답\n","      - 추출\n","      - 수학적 추론\n","      - 독해\n","      - 번역\n","      - 표현\n","    - 평가 데이터의 유출\n","    - 전문 능력\n","    - 정렬\n","      - 제어성\n","      - 윤리·도덕\n","      - 유해성\n","      - 편향\n","      - 진실성\n","      - 견고성\n","  - How to evaluate: 평가 방법\n","    - 기반 모델의 평가 vs 파인튜닝된 모델의 평가\n","    - 평가의 단계\n","      - 프롬프트 설계\n","      - Few-shot vs Zero-shot\n","      - 로그 우도 선택 방식과 직접 생성 방식\n","      - 출력의 후처리와 정답의 일의성 담보\n","    - 평가 산출 방법\n","      - 일반적인 벤치마크에 의한 평가\n","- 본 백서에서 제안하는 체계\n","  - 범용 언어 성능\n","  - AI 거버넌스 도메인 특화 성능\n","  - 응용 능력\n","    - 표현\n","    - 번역\n","    - 독해\n","    - 수학적 배경 추출\n","    - 지식, 깊은 응답\n","  - 자연어 추론 자연어 이해\n","    - 기초적 언어 능력\n","    - 전문 능력\n","  - 법률\n","  - 의료\n","  - 금융\n","  - 프로그래밍\n","  - 그 외 기타\n","  - 얼라인먼트\n","    - 제어성\n","    - 윤리·도덕\n","    - 유해성\n","    - 편향성\n","    - 진실성\n","    - 견고성\n","  - 시스템 성능\n","    - 추론 속도\n","    - 지론 호출\n","  - 시큐리티\n","- 그림 4: 본 백서에서 제안하는 LLM 평가의 전체 모습과 체계\n"]}],"source":["print(rag_chain.invoke(\"문서의 목차를 알려줘\"))"]},{"cell_type":"markdown","metadata":{"id":"ECo58aXUaHhm"},"source":["## RAG 챗봇 (과거 질문답변 기억)"]},{"cell_type":"markdown","metadata":{"id":"u7lY5KXhaRZh"},"source":["벡터DB, retriever, LLM을 불러오는 것은 위와 동일합니다."]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1716903315758,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"4QlzHqFxaRZi"},"outputs":[],"source":["from langchain_chroma import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","\n","vectorstore = Chroma(\n","    persist_directory=\"./db\",\n","    embedding_function=OpenAIEmbeddings())\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1716903315758,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"jzMqZNxSaRZi"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")"]},{"cell_type":"markdown","metadata":{"id":"iiKo_yB2aeXE"},"source":["문서를 벡터DB에서 가져올 때는 질문만 보기 때문에 해당 질문이 과거의 질문답변 내용을 반영하고 있도록 수정해야 합니다.\n","\n","LLM을 사용해서 질문을 수정하고, 수정된 질문으로 벡터DB에서 유사한 문서를 가져오는 단계입니다."]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":419,"status":"ok","timestamp":1716903451545,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"lhG4xmgdaYX7"},"outputs":[],"source":["from langchain.chains import create_history_aware_retriever\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n","which might reference context in the chat history, formulate a standalone question \\\n","which can be understood without the chat history. Do NOT answer the question, \\\n","just reformulate it if needed and otherwise return it as is.\"\"\"\n","contextualize_q_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", contextualize_q_system_prompt),\n","        MessagesPlaceholder(\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","history_aware_retriever = create_history_aware_retriever(\n","    llm, retriever, contextualize_q_prompt\n",")"]},{"cell_type":"markdown","metadata":{"id":"18j5akNvbB2t"},"source":["사용자 질문, 문맥이 되는 문서, 과거의 질문답변 내용을 입력하고 LLM으로 질문에 대한 답변을 생성하는 단계입니다."]},{"cell_type":"code","execution_count":65,"metadata":{"executionInfo":{"elapsed":310,"status":"ok","timestamp":1716903701299,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"w524ky55a5b1"},"outputs":[],"source":["from langchain.chains import create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","\n","qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n","Use the following pieces of retrieved context to answer the question. \\\n","If you don't know the answer, just say that you don't know. \\\n","Use three sentences maximum and keep the answer concise.\\\n","\n","{context}\"\"\"\n","qa_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", qa_system_prompt),\n","        MessagesPlaceholder(\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"]},{"cell_type":"markdown","metadata":{"id":"b5MmaMc9bd5v"},"source":["위 단계를 연결해서 `history_rag_chain`을 만듭니다\n","\n","1. 미리 모든 문서를 청크로 분리하고 임베딩해서 벡터DB에 저장합니다.\n","2. 사용자한테 질문을 입력받습니다.\n","3. 과거 질문답변 내역 없이도 질문에 답변할 수 있도록 새로운 질문으로 만들어달라고 LLM에 요청합니다.\n","3. 새로운 질문과 비슷한 문서 청크를 벡터DB에서 찾아 가져옵니다.\n","4. 가져온 문서 청크와 과거 질문답변 내역을 기반으로 사용자 질문에 대한 답변을 LLM이 생성합니다."]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1716903706831,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"zo6fBaY3bdGd"},"outputs":[],"source":["from langchain.chains import create_retrieval_chain\n","\n","history_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"]},{"cell_type":"markdown","metadata":{"id":"ZGd8i5Tncn8w"},"source":["임시로 과거 질문답변을 저장할 수 있는 공간을 만들기 위해 `ChatMessageHistory`를 사용합니다.\n","\n","(앱에서는 `StreamlitChatMessageHistory`을 사용합니다.)\n","\n","`RunnableWithMessageHistory`로 질문을 할 때 자동으로 질문답변 내역을 기록합니다."]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":399,"status":"ok","timestamp":1716904195705,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"hpW0cUXMbz5J"},"outputs":[],"source":["from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.chat_history import BaseChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","\n","### Statefully manage chat history ###\n","store = {}\n","\n","\n","def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]\n","\n","\n","conversational_rag_chain = RunnableWithMessageHistory(\n","    history_rag_chain,\n","    get_session_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"chat_history\",\n","    output_messages_key=\"answer\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"5bloVqijfl4I"},"source":["잘 답변하는지 확인해 봅시다."]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8923,"status":"ok","timestamp":1716904525257,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"rKKGEUuYdC-H","outputId":"2e339741-508b-4e84-fc6e-165bb35b17aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["######## 첫 번째 질문: 문서의 목차를 알려줘 ############\n","본 문서의 목차는 다음과 같습니다:\n","1. 서론\n","   - 본 백서 작성의 배경\n","   - LLM을 왜 평가하는가?\n","   - 백서의 대상 독자\n","   - LLM 평가의 프레임워크와 용어 정리\n","2. 언어 모델 평가의 전체 모습\n","   - 본 백서에서 제안하는 체계\n","3. LLM 모델의 평가 방법\n","   - What to evaluate: 평가할 측면\n","   - How to evaluate: 평가 방법\n","      - 기반 모델의 평가 vs 파인튜닝된 모델의 평가\n","      - 평가의 단계\n","      - 평가 산출 방법\n","4. 그림 4: 본 백서에서 제안하는 LLM 평가의 전체 모습과 체계\n","\n","######## 두 번째 질문: 첫 번째 항목에 대해 설명해줘. ############\n","첫 번째 항목은 \"서론\"입니다. 이 항목에서는 본 백서 작성의 배경, LLM을 평가하는 이유, 대상 독자, 그리고 LLM 평가를 위한 프레임워크와 용어에 대한 정리가 포함됩니다. 여기서는 왜 LLM을 평가해야 하는지와 이를 어떤 관점에서 다룰 것인지에 대한 내용이 다루어집니다.\n"]}],"source":["get_session_history(\"test\").clear() # 과거내역을 삭제합니다.\n","print(\"######## 첫 번째 질문: 문서의 목차를 알려줘 ############\")\n","response = conversational_rag_chain.invoke(\n","    {\"input\": \"문서의 목차를 알려줘\"},\n","    config={\n","        \"configurable\": {\"session_id\": \"test\"}\n","    },\n",")[\"answer\"]\n","print(response)\n","print()\n","\n","print(\"######## 두 번째 질문: 첫 번째 항목에 대해 설명해줘. ############\")\n","response = conversational_rag_chain.invoke(\n","    {\"input\": \"첫 번째 항목에 대해 설명해줘.\"},\n","    config={\n","        \"configurable\": {\"session_id\": \"test\"}\n","    },\n",")[\"answer\"]\n","print(response)"]},{"cell_type":"markdown","metadata":{"id":"UbS5qabzepJn"},"source":["과거 질문답변이 저장되는 것을 볼 수 있습니다."]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1716904532362,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"RoNAOyMabyef","outputId":"cde7d6b9-f2f7-40ec-a48b-1a18348c24c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Human: 문서의 목차를 알려줘\n","AI: 본 문서의 목차는 다음과 같습니다:\n","1. 서론\n","   - 본 백서 작성의 배경\n","   - LLM을 왜 평가하는가?\n","   - 백서의 대상 독자\n","   - LLM 평가의 프레임워크와 용어 정리\n","2. 언어 모델 평가의 전체 모습\n","   - 본 백서에서 제안하는 체계\n","3. LLM 모델의 평가 방법\n","   - What to evaluate: 평가할 측면\n","   - How to evaluate: 평가 방법\n","      - 기반 모델의 평가 vs 파인튜닝된 모델의 평가\n","      - 평가의 단계\n","      - 평가 산출 방법\n","4. 그림 4: 본 백서에서 제안하는 LLM 평가의 전체 모습과 체계\n","Human: 첫 번째 항목에 대해 설명해줘.\n","AI: 첫 번째 항목은 \"서론\"입니다. 이 항목에서는 본 백서 작성의 배경, LLM을 평가하는 이유, 대상 독자, 그리고 LLM 평가를 위한 프레임워크와 용어에 대한 정리가 포함됩니다. 여기서는 왜 LLM을 평가해야 하는지와 이를 어떤 관점에서 다룰 것인지에 대한 내용이 다루어집니다.\n"]}],"source":["print(get_session_history(\"test\"))"]},{"cell_type":"markdown","metadata":{"id":"AMeaiJJXeO0r"},"source":["반면 과거 질문답변을 기억하지 않는 챗봇은 엉뚱한 답을 낼 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16352,"status":"ok","timestamp":1716904552683,"user":{"displayName":"Jieun Kwon","userId":"08923377727647414822"},"user_tz":-540},"id":"mW7_WTyfeTHf","outputId":"1f5ec240-bf1d-4322-b107-7f0387615716"},"outputs":[{"name":"stdout","output_type":"stream","text":["######## 첫 번째 질문: 문서의 목차를 알려줘 ############\n","- 목차\n","- 목차\n","- 서론\n","  - 본 백서 작성의 배경\n","    - LLM을 왜 평가하는가?\n","  - 백서의 대상 독자\n","  - LLM 평가의 프레임워크와 용어 정리\n","- 언어 모델 평가의 전체 모습\n","  - 본 백서에서 제안하는 체계\n","- LLM 모델의 평가 방법\n","  - What to evaluate: 평가할 측면\n","    - 기초 언어 능력\n","      - 언어 이해(Natural Language Understanding)\n","      - 자연어 추론(Natural Language Inference)\n","    - 응용 능력(Application Skills)\n","      - 지식·질의응답(Knowledge / Question Answering)\n","      - 추출(Extraction)\n","      - 수학적 추론(Mathematical Reasoning)\n","      - 독해(Reading Comprehension)\n","      - 번역(Translation)\n","      - 표현(Expression)\n","    - 평가 데이터의 유출\n","    - 전문 능력\n","    - 정렬(Alignment)\n","      - 제어성(Controllability)\n","      - 윤리·도덕(Ethics / Morality)\n","      - 유해성(Toxicity)\n","      - 편향(Bias)\n","      - 진실성(Truthfulness)\n","      - 견고성(Robustness)\n","  - How to evaluate: 평가 방법\n","    - 기반 모델의 평가 vs 파인튜닝된 모델의 평가\n","    - 평가의 단계\n","      - 프롬프트 설계\n","      - Few-shot vs Zero-shot\n","      - 로그 우도 선택 방식과 직접 생성 방식\n","      - 출력의 후처리와 정답의 일의성 담보\n","    - 평가 산출 방법\n","      - 일반적인 벤치마크에 의한 평가\n","- 본 백서에서 제안하는 체계\n","  - 범용 언어 성능\n","  - AI 거버넌스도메인 특화 성능\n","  - 응용 능력\n","    - 표현\n","    - 번역\n","    - 독해\n","    - 수학적 배경 추출\n","    - 지식, 깊은 응답\n","  - 자연어 추론 자연어 이해\n","    - 기초적 언어 능력\n","  - 전문 능력\n","    - 법률\n","    - 의료\n","    - 금융\n","    - 프로그래밍\n","    - 그 외 기타\n","  - 얼라인먼트\n","    - 제어성\n","    - 윤리·도덕\n","    - 유해성\n","    - 편향성\n","    - 진실성\n","    - 견고성\n","  - 시스템 성능\n","    - 추론 속도\n","    - 지론 호출\n","  - 시큐리티\n","- 그림 4: 본 백서에서 제안하는 LLM 평가의 전체 모습과 체계\n","\n","######## 두 번째 질문: 첫 번째 항목에 대해 설명해줘. ############\n","2: 남자는 신용카드로 티켓을 결제한다.\n"]}],"source":["print(\"######## 첫 번째 질문: 문서의 목차를 알려줘 ############\")\n","print(rag_chain.invoke(\"문서의 목차를 알려줘\"))\n","print()\n","print(\"######## 두 번째 질문: 첫 번째 항목에 대해 설명해줘. ############\")\n","print(rag_chain.invoke(\"첫 번째 항목에 대해 설명해줘.\"))"]},{"cell_type":"markdown","metadata":{"id":"4LDAhbFo2rcc"},"source":["# 챗봇 답변 성능 높이는 방법\n","\n","- LLM에 입력할 프롬프트 개선\n","- 문서를 벡터DB에 넣기 전에 청크로 나누는 방법 변경 (chunk_size, chunk_overlap)\n","- 문서 가져오는(retrieve) 방법 개선\n","- 문서 가져온(retrieve) 후 순위 조정(reranking) 단계 추가\n","- 더 성능 좋은 모델로 변경 (gpt4)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOtjYK83QD6sVUjraFO8lhn","collapsed_sections":["fvArA65nrFIn","PoKTGlPpWxXa"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
